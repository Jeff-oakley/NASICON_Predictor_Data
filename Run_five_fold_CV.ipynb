{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two set of metrics are demonstrated for all data\n",
    "1. 80-20 validation, with validation error demonstrated as peformance metric. The 80-20 validation is used for selecting the 2D descriptors\n",
    "2. five-fold cross validation, with average CV error demonstrated as performance metric\n",
    "\n",
    "Note: The Ehull value shows in the data are actually Ehull (1000K), this is calculated by Ehull(1000K) = Ehull(DFT) - S_ideal*1000K. The reason of doing that is because we use Ehull(1000K) to seperate GS/LS with US NASICONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pymatgen.core.composition import Composition\n",
    "\n",
    "\n",
    "prefix = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, the train test data here are from 80-20 data splitting. Which is used for SIS+MLR screening of 2D descriptors\n",
    "\n",
    "train_2D_df = pd.read_csv('train_2D.csv')\n",
    "test_2D_df = pd.read_csv('test_2D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CompName</th>\n",
       "      <th>Ehull</th>\n",
       "      <th>(cbrt(NNaLst)+(AnionChgStdLst)^2)</th>\n",
       "      <th>((EWaldSumLst)^2*(XWithNaLst*RDiffStdLst))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MgZr(SO4)3</td>\n",
       "      <td>-60.260567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MgTi(SO4)3</td>\n",
       "      <td>-27.986572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.960243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MgSn(SO4)3</td>\n",
       "      <td>-30.640966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.109451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mg4Nb2(SO4)9</td>\n",
       "      <td>-42.783127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.651306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZrZn(SO4)3</td>\n",
       "      <td>-40.419098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.253452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CompName      Ehull  (cbrt(NNaLst)+(AnionChgStdLst)^2)  \\\n",
       "0    MgZr(SO4)3 -60.260567                                0.0   \n",
       "1    MgTi(SO4)3 -27.986572                                0.0   \n",
       "2    MgSn(SO4)3 -30.640966                                0.0   \n",
       "3  Mg4Nb2(SO4)9 -42.783127                                0.0   \n",
       "4    ZrZn(SO4)3 -40.419098                                0.0   \n",
       "\n",
       "   ((EWaldSumLst)^2*(XWithNaLst*RDiffStdLst))  \n",
       "0                                    0.000000  \n",
       "1                                  125.960243  \n",
       "2                                   46.109451  \n",
       "3                                   79.651306  \n",
       "4                                   24.253452  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_2D_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "Ehull_train = train_2D_df['Ehull'].to_numpy()\n",
    "Y_total = np.zeros_like(Ehull_train)\n",
    "Y_total[np.where(Ehull_train<=0)] = 0\n",
    "Y_total[np.where(Ehull_train>0)] = 1\n",
    "X_total = train_2D_df[['(cbrt(NNaLst)+(AnionChgStdLst)^2)','((EWaldSumLst)^2*(XWithNaLst*RDiffStdLst))']].to_numpy()\n",
    "\n",
    "Ehull_test = test_2D_df['Ehull'].to_numpy()\n",
    "Y_valid = np.zeros_like(Ehull_test)\n",
    "Y_valid[np.where(Ehull_test<=0)] = 0\n",
    "Y_valid[np.where(Ehull_test>0)] = 1\n",
    "X_valid = test_2D_df[['(cbrt(NNaLst)+(AnionChgStdLst)^2)','((EWaldSumLst)^2*(XWithNaLst*RDiffStdLst))']].to_numpy()\n",
    "print(np.unique(Y_total),np.unique(Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric on 20% of the validation data (out of the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSGS recall = 0.796875, US recall = 0.8289676425269645\n",
      "LSGS F1 = 0.5982404692082111, US F1 = 0.8870568837592745\n",
      "Accuracy = 0.8236808236808236, recall = 0.8129213212634823, F1 score = 0.7426486764837428\n"
     ]
    }
   ],
   "source": [
    "# The SVC validation error from 80-20 splitting\n",
    "\n",
    "clf = SVC(kernel='linear',class_weight='balanced',probability=True)\n",
    "clf.fit(X_total,Y_total)\n",
    "Y_valid_predict = clf.predict(X_valid)\n",
    "test_score = accuracy_score(Y_valid,Y_valid_predict)\n",
    "test_recall_avg = recall_score(Y_valid,Y_valid_predict,average='macro')\n",
    "test_recall_LSGS = recall_score(Y_valid,Y_valid_predict,pos_label=0)\n",
    "test_recall_US = recall_score(Y_valid,Y_valid_predict,pos_label=1)\n",
    "test_F1_avg = f1_score(Y_valid,Y_valid_predict,average='macro')\n",
    "test_F1_LSGS = f1_score(Y_valid,Y_valid_predict,pos_label=0)\n",
    "test_F1_US = f1_score(Y_valid,Y_valid_predict,pos_label=1)\n",
    "\n",
    "print(f'LSGS recall = {test_recall_LSGS}, US recall = {test_recall_US}')\n",
    "print(f'LSGS F1 = {test_F1_LSGS}, US F1 = {test_F1_US}')\n",
    "print(f'Accuracy = {test_score}, recall = {test_recall_avg}, F1 score = {test_F1_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric on five fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_data = np.concatenate((X_total,X_valid))\n",
    "Y_all_data = np.concatenate((Y_total,Y_valid))\n",
    "ehull_all_data = np.concatenate((Ehull_train,Ehull_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3881,) (3881, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_all_data.shape,X_all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "LSGS recall = 0.875968992248062, US recall = 0.8209876543209876\n",
      "LSGS F1 = 0.6312849162011173, US F1 = 0.8896321070234113\n",
      "Accuracy = 0.8301158301158301, average recall = 0.8484783232845248, average F1 score = 0.7604585116122643\n",
      "LSGS recall = 0.875968992248062, US recall = 0.8209876543209876\n",
      "LSGS F1 = 0.6312849162011173, US F1 = 0.8896321070234113\n",
      "Accuracy = 0.8301158301158301, average recall = 0.8484783232845248, average F1 score = 0.7604585116122643\n",
      "===============================\n",
      "LSGS recall = 0.828125, US recall = 0.8024691358024691\n",
      "LSGS F1 = 0.5856353591160222, US F1 = 0.8739495798319329\n",
      "Accuracy = 0.8067010309278351, average recall = 0.8152970679012346, average F1 score = 0.7297924694739775\n",
      "LSGS recall = 0.828125, US recall = 0.8024691358024691\n",
      "LSGS F1 = 0.5856353591160222, US F1 = 0.8739495798319329\n",
      "Accuracy = 0.8067010309278351, average recall = 0.8152970679012346, average F1 score = 0.7297924694739775\n",
      "===============================\n",
      "LSGS recall = 0.8125, US recall = 0.8132716049382716\n",
      "LSGS F1 = 0.5892351274787536, US F1 = 0.8790658882402\n",
      "Accuracy = 0.8131443298969072, average recall = 0.8128858024691358, average F1 score = 0.7341505078594768\n",
      "LSGS recall = 0.8125, US recall = 0.8132716049382716\n",
      "LSGS F1 = 0.5892351274787536, US F1 = 0.8790658882402\n",
      "Accuracy = 0.8131443298969072, average recall = 0.8128858024691358, average F1 score = 0.7341505078594768\n",
      "===============================\n",
      "LSGS recall = 0.921875, US recall = 0.8302469135802469\n",
      "LSGS F1 = 0.6629213483146068, US F1 = 0.899665551839465\n",
      "Accuracy = 0.845360824742268, average recall = 0.8760609567901234, average F1 score = 0.7812934500770359\n",
      "LSGS recall = 0.921875, US recall = 0.8302469135802469\n",
      "LSGS F1 = 0.6629213483146068, US F1 = 0.899665551839465\n",
      "Accuracy = 0.845360824742268, average recall = 0.8760609567901234, average F1 score = 0.7812934500770359\n",
      "===============================\n",
      "LSGS recall = 0.8125, US recall = 0.808641975308642\n",
      "LSGS F1 = 0.5842696629213483, US F1 = 0.8762541806020067\n",
      "Accuracy = 0.8092783505154639, average recall = 0.810570987654321, average F1 score = 0.7302619217616775\n",
      "LSGS recall = 0.8125, US recall = 0.808641975308642\n",
      "LSGS F1 = 0.5842696629213483, US F1 = 0.8762541806020067\n",
      "Accuracy = 0.8092783505154639, average recall = 0.810570987654321, average F1 score = 0.7302619217616775\n",
      "Average accuracy = 0.820920073239661, recall = 0.8326586276198679, F1 score = 0.7471913721568864\n"
     ]
    }
   ],
   "source": [
    "# The real train set = train set from skf + the validation data set\n",
    "accuracy_lst, recall_lst, F1_lst = [], [], []\n",
    "\n",
    "best_test_score = 1e10\n",
    "\n",
    "for train_index, test_index in skf.split(X_all_data,Y_all_data):\n",
    "    print(\"===============================\")\n",
    "    X_train = X_all_data[train_index]\n",
    "    X_test = X_all_data[test_index]\n",
    "    Y_train = Y_all_data[train_index]\n",
    "    Y_test = Y_all_data[test_index]\n",
    "    \n",
    "    clf = SVC(kernel='linear',class_weight='balanced',probability=True)\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_cv_predict = clf.predict(X_test)\n",
    "    \n",
    "    test_score = accuracy_score(Y_test,Y_cv_predict)\n",
    "    test_recall_avg = recall_score(Y_test,Y_cv_predict,average='macro')\n",
    "    test_recall_LSGS = recall_score(Y_test,Y_cv_predict,pos_label=0)\n",
    "    test_recall_US = recall_score(Y_test,Y_cv_predict,pos_label=1)\n",
    "    test_F1 = f1_score(Y_test,Y_cv_predict,average='macro')\n",
    "    test_F1_LSGS = f1_score(Y_test,Y_cv_predict,pos_label=0)\n",
    "    test_F1_US = f1_score(Y_test,Y_cv_predict,pos_label=1)\n",
    "    \n",
    "    accuracy_lst.append(test_score)\n",
    "    recall_lst.append(test_recall_avg)\n",
    "    F1_lst.append(test_F1)\n",
    "    \n",
    "    print(f'LSGS recall = {test_recall_LSGS}, US recall = {test_recall_US}')\n",
    "    print(f'LSGS F1 = {test_F1_LSGS}, US F1 = {test_F1_US}')\n",
    "    print(f'Accuracy = {test_score}, average recall = {test_recall_avg}, average F1 score = {test_F1}')\n",
    "    \n",
    "    if test_score < best_test_score:\n",
    "        best_test_score = test_score\n",
    "        best_clf = deepcopy(clf)\n",
    "        best_train_index, best_test_index = deepcopy(train_index), deepcopy(test_index)\n",
    "        best_X_train, best_Y_train = deepcopy(X_train), deepcopy(Y_train)\n",
    "        best_X_test, best_Y_test = deepcopy(X_test), deepcopy(Y_test)\n",
    "    \n",
    "    print(f'LSGS recall = {test_recall_LSGS}, US recall = {test_recall_US}')\n",
    "    print(f'LSGS F1 = {test_F1_LSGS}, US F1 = {test_F1_US}')\n",
    "    print(f'Accuracy = {test_score}, average recall = {test_recall_avg}, average F1 score = {test_F1}')\n",
    "    \n",
    "print(f'Average accuracy = {np.average(accuracy_lst)}, recall = {np.average(recall_lst)}, F1 score = {np.average(F1_lst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8152970679012346 0.828125 0.7978395061728395\n",
      "0.828125\n"
     ]
    }
   ],
   "source": [
    "# Obtain recall value on LS-GS NASICONs \n",
    "\n",
    "X_test = X_all_data[best_test_index]\n",
    "Y_test = Y_all_data[best_test_index]\n",
    "Y_predict = best_clf.predict(X_test)\n",
    "test_recall_avg = recall_score(Y_test,Y_predict,average='macro')\n",
    "test_recall_LSGS = recall_score(Y_test,Y_predict,pos_label=0)\n",
    "test_recall_US = recall_score(Y_test,Y_cv_predict,pos_label=1)\n",
    "print(test_recall_avg,test_recall_LSGS,test_recall_US)\n",
    "print(np.where((Y_test==Y_predict) & (Y_test==0))[0].shape[0]/np.where(Y_test==0)[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.59431814 0.01023764]]\n",
      "[-5.75680291]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'norm_factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4f15e7d8b217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm_factor\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm_factor\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_factor' is not defined"
     ]
    }
   ],
   "source": [
    "# print model coeffcient\n",
    "\n",
    "print(best_clf.coef_)\n",
    "print(best_clf.intercept_)\n",
    "x_coeff = best_clf.coef_[0][0]*norm_factor/best_clf.coef_[0][1]\n",
    "y_coeff = 1.0\n",
    "intercept = -best_clf.intercept_[0]*norm_factor/best_clf.coef_[0][1]\n",
    "print(f'{x_coeff}x+y<={intercept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all the data with the decision boundary that has highest validation accuracy/F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the normalization factor to make sure the second feature dimensionaless\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df = train_df.set_index('CompName')\n",
    "test_df = test_df.set_index('CompName')\n",
    "\n",
    "e_Ewald_ref = train_df.loc['Na3Zr2Si2PO12']['EWaldSumLst']\n",
    "r_ref = 1.02 # The shannon radius of Na+\n",
    "\n",
    "norm_factor = 1/(e_Ewald_ref**2*r_ref)\n",
    "print(f'norm_factor is {norm_factor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "x_min, x_max = best_X_train[:,0].min()-0.5, best_X_train[:, 0].max()+0.5\n",
    "y_min, y_max = best_X_train[:,1].min()-50, best_X_train[:, 1].max()+100\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)\n",
    "\n",
    "plot_stepx = (x_max-x_min)/500;\n",
    "plot_stepy = (y_max-y_min)/500;\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_stepx),\n",
    "                     np.arange(y_min, y_max, plot_stepy))\n",
    "Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.binary,alpha=0.2)\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "print(best_X_train.shape,ehull_train_arry.shape)\n",
    "\n",
    "plt.scatter(best_X_train[:,0],best_X_train[:,1],marker='o',s=[60 for i in range(ehull_train_arry.shape[0])],\n",
    "            c=ehull_train_arry,cmap='RdYlGn_r',vmin=-20,vmax=20,alpha=0.6)\n",
    "plt.scatter(best_X_test[:,0],best_X_test[:,1],marker='s',s=[60 for i in range(ehull_test_arry.shape[0])],\n",
    "            c=ehull_test_arry,edgecolors=['k' for i in range(ehull_train_arry.shape[0])],\n",
    "            cmap='RdYlGn_r',vmin=-20,vmax=20,alpha=0.8)\n",
    "\n",
    "\n",
    "CB=plt.colorbar()\n",
    "CB.ax.tick_params(labelsize=24)\n",
    "CB.set_ticks([-20,0,20]); CB.set_ticklabels(['-20','0','20'])\n",
    "_ = plt.yticks([0.00/norm_factor,0.10/norm_factor,0.20/norm_factor,0.30/norm_factor,0.40/norm_factor],\n",
    "           [0.0,0.1,0.2,0.3,0.4])\n",
    "_ = plt.xticks(fontsize=24); plt.yticks(fontsize=24)\n",
    "plt.xlabel('$\\sqrt[3]{N_{Na}}$+($Q_{A}^{Std}$)$^2$',fontsize=28)\n",
    "plt.ylabel(r'E$_{EWald}^2$$\\cdot$X$_{M}^{Na}$$\\cdot$R$_{M}^{Std}$',fontsize=28)\n",
    "plt.xlim((-0.5,2.8)) \n",
    "plt.ylim((-30,780))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_5a.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More performance metrics of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot experimentally reported NASICONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('exp_comps.json','r') as fid:\n",
    "    exp_comp_dict = json.load(fid)\n",
    "    \n",
    "reported_exp_comps = exp_comp_dict['reported_exp_comps']\n",
    "new_exp_comps = exp_comp_dict['new_exp_comps']\n",
    "\n",
    "all_comp_names = train_2D_df['CompName'].to_list() + test_2D_df['CompName'].tolist()\n",
    "\n",
    "reported_exp_inds = [all_comp_names.index(exp_comp) for exp_comp in reported_exp_comps]\n",
    "new_exp_inds = [all_comp_names.index(exp_comp) for exp_comp in new_exp_comps]\n",
    "\n",
    "print(len(reported_exp_comps), len(new_exp_comps))\n",
    "print(len(reported_exp_inds), len(new_exp_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "x_min, x_max = best_X_train[:,0].min()-0.5, best_X_train[:, 0].max()+0.5\n",
    "y_min, y_max = best_X_train[:,1].min()-50, best_X_train[:, 1].max()+100\n",
    "\n",
    "plot_stepx = (x_max-x_min)/500;\n",
    "plot_stepy = (y_max-y_min)/500;\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_stepx),\n",
    "                     np.arange(y_min, y_max, plot_stepy))\n",
    "Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.binary,alpha=0.2)\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "\n",
    "plt.plot(X_all_data[reported_exp_inds,0],X_all_data[reported_exp_inds,1],'*',fillstyle='none',\n",
    "         color='k',markersize=30,label='Reported NASICON',alpha=1.0,markeredgewidth=2)\n",
    "plt.plot(X_all_data[new_exp_inds,0],X_all_data[new_exp_inds,1],'*',fillstyle='full',color='g',markersize=25,\\\n",
    "         label='This Work',alpha=1.0,markeredgewidth=2)\n",
    "_ = plt.yticks([0.00/norm_factor,0.10/norm_factor,0.20/norm_factor,0.30/norm_factor,0.40/norm_factor],\n",
    "           [0.0,0.1,0.2,0.3,0.4])\n",
    "_ = plt.xticks(fontsize=24); plt.yticks(fontsize=24)\n",
    "plt.xlabel('$\\sqrt[3]{N_{Na}}$+($Q_{A}^{Std}$)$^2$',fontsize=28)\n",
    "plt.ylabel(r'E$_{EWald}^2$$\\cdot$X$_{M}^{Na}$$\\cdot$R$_{M}^{Std}$',fontsize=28)\n",
    "plt.xlim((-0.5,2.8)) \n",
    "plt.ylim((-30,780))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_5b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in the five newly synthesized compounds\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "x_min, x_max = best_X_train[:,0].min()-0.5, best_X_train[:, 0].max()+0.5\n",
    "y_min, y_max = best_X_train[:,1].min()-50, best_X_train[:, 1].max()+100\n",
    "\n",
    "plot_stepx = (x_max-x_min)/500;\n",
    "plot_stepy = (y_max-y_min)/500;\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_stepx),\n",
    "                     np.arange(y_min, y_max, plot_stepy))\n",
    "Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.binary,alpha=0.2)\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "\n",
    "plt.plot(X_all_data[reported_exp_inds,0],X_all_data[reported_exp_inds,1],'^',fillstyle='full',color='k',markersize=36,label='Reported NASICON',\\\n",
    "         alpha=0.3,markeredgewidth=2)\n",
    "plt.plot(X_all_data[new_exp_inds,0],X_all_data[new_exp_inds,1],'v',fillstyle='full',color='g',markersize=36,\\\n",
    "         label='This Work',alpha=0.3,markeredgewidth=2,markeredgecolor='g')\n",
    "plt.xlim((1.62,1.72)); plt.ylim((-3,18));\n",
    "plt.xticks([1.65,1.7],[1.65,1.7],fontsize=24);\n",
    "plt.yticks([0.00/norm_factor,0.004/norm_factor,0.008/norm_factor],\n",
    "           [0.0,0.004,0.008],fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_5b_zoom.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot Na rich and Na poor NASICONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_NNa(comp_lst):\n",
    "    NNa_lst = []\n",
    "    for reduced_formula in comp_lst:\n",
    "        NNa = Composition(reduced_formula)['Na']/Composition(reduced_formula)['O']*12.0\n",
    "        NNa_lst.append(NNa)\n",
    "        \n",
    "    return np.array(NNa_lst)\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "train_comp_names = [all_comp_names[i] for i in best_train_index]\n",
    "test_comp_names = [all_comp_names[i] for i in best_test_index]\n",
    "\n",
    "NNa_train_arry = obtain_NNa(train_comp_names)\n",
    "NNa_test_arry = obtain_NNa(test_comp_names)\n",
    "\n",
    "NaPoor_LSGS_train_ind = np.where((NNa_train_arry<=1.0) & (ehull_train_arry<=0))[0]\n",
    "NaRich_LSGS_train_ind = np.where((NNa_train_arry>=3.0) & (ehull_train_arry<=0))[0]\n",
    "NaPoor_LSGS_test_ind = np.where((NNa_test_arry<=1.0) & (ehull_test_arry<=0))[0]\n",
    "NaRich_LSGS_test_ind = np.where((NNa_test_arry>=3.0) & (ehull_test_arry<=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weighted ratio\n",
    "\n",
    "ratio =  best_clf.class_weight_/best_clf.class_weight_.sum()\n",
    "NaPoor_LSGS_train_proba = best_clf.predict_proba(best_X_train[NaPoor_LSGS_train_ind])*ratio\n",
    "NaRich_LSGS_train_proba = best_clf.predict_proba(best_X_train[NaRich_LSGS_train_ind])*ratio\n",
    "NaPoor_LSGS_test_proba = best_clf.predict_proba(best_X_test[NaPoor_LSGS_test_ind])*ratio\n",
    "NaRich_LSGS_test_proba = best_clf.predict_proba(best_X_test[NaRich_LSGS_test_ind])*ratio\n",
    "\n",
    "print(\n",
    "    NaPoor_LSGS_train_proba.shape, NaRich_LSGS_train_proba.shape, \\\n",
    "    NaPoor_LSGS_test_proba.shape, NaRich_LSGS_test_proba.shape, \\\n",
    "    len(NaPoor_LSGS_train_ind), len(NaRich_LSGS_train_ind), \\\n",
    "    len(NaPoor_LSGS_test_ind), len(NaRich_LSGS_test_ind)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "x_min, x_max = best_X_train[:,0].min()-0.5, best_X_train[:, 0].max()+0.5\n",
    "y_min, y_max = best_X_train[:,1].min()-50, best_X_train[:, 1].max()+100\n",
    "\n",
    "plot_stepx = (x_max-x_min)/500;\n",
    "plot_stepy = (y_max-y_min)/500;\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_stepx),\n",
    "                     np.arange(y_min, y_max, plot_stepy))\n",
    "Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.binary,alpha=0.2)\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "\n",
    "plt.scatter(best_X_train[NaPoor_LSGS_train_ind,0],best_X_train[NaPoor_LSGS_train_ind,1],cmap='pink_r',marker='o',\n",
    "         s=210,c=NaPoor_LSGS_train_proba[:,0],edgecolors=['g' for _ in range(len(NaPoor_LSGS_test_ind))],alpha=0.4,\n",
    "            vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_train[NaRich_LSGS_train_ind,0],best_X_train[NaRich_LSGS_train_ind,1],cmap='pink_r',marker='^',\n",
    "         s=210,c=NaRich_LSGS_train_proba[:,0],edgecolors=['g' for _ in range(len(NaPoor_LSGS_test_ind))],alpha=0.4,\n",
    "            vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_test[NaPoor_LSGS_test_ind,0],best_X_test[NaPoor_LSGS_test_ind,1],cmap='pink_r',marker='o',\n",
    "         s=210,c=NaPoor_LSGS_test_proba[:,0],edgecolors=['k' for _ in range(len(NaPoor_LSGS_test_ind))],alpha=0.8,\n",
    "         vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_test[NaRich_LSGS_test_ind,0],best_X_test[NaRich_LSGS_test_ind,1],cmap='pink_r',marker='^',\n",
    "         s=210,c=NaRich_LSGS_test_proba[:,0],edgecolors=['k' for _ in range(len(NaRich_LSGS_test_ind))],alpha=0.8,\n",
    "         vmin=0.0,vmax=1.0)\n",
    "\n",
    "_ = plt.yticks([0.00/norm_factor,0.10/norm_factor,0.20/norm_factor,0.30/norm_factor,0.40/norm_factor],\n",
    "           [0.0,0.1,0.2,0.3,0.4])\n",
    "_ = plt.xticks(fontsize=24); plt.yticks(fontsize=24)\n",
    "plt.xlabel('$\\sqrt[3]{N_{Na}}$+($Q_{A}^{Std}$)$^2$',fontsize=28)\n",
    "plt.ylabel(r'E$_{EWald}^2$$\\cdot$X$_{M}^{Na}$$\\cdot$R$_{M}^{Std}$',fontsize=28)\n",
    "plt.xlim((-0.5,2.8)) \n",
    "plt.ylim((-30,780))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_5c.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot Ca/Ge and Hf/Zr NASICONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_NASICON_with_metal(comp_lst,ehull_arry,metals):\n",
    "    NASICON_comp_lst, NASICON_comp_inds = [], []\n",
    "    for i, reduced_formula in enumerate(comp_lst):\n",
    "        NM = 0.0\n",
    "        for metal in metals:\n",
    "            NM += Composition(reduced_formula)[metal]/Composition(reduced_formula)['O']*12.0\n",
    "        if NM > 0.0 and ehull_arry[i] <= 0:\n",
    "            NASICON_comp_lst.append(reduced_formula)\n",
    "            NASICON_comp_inds.append(i)\n",
    "        \n",
    "    return NASICON_comp_lst, NASICON_comp_inds\n",
    "\n",
    "\n",
    "ehull_train_arry = ehull_all_data[best_train_index]\n",
    "ehull_test_arry = ehull_all_data[best_test_index]\n",
    "train_comp_names = [all_comp_names[i] for i in best_train_index]\n",
    "test_comp_names = [all_comp_names[i] for i in best_test_index]\n",
    "\n",
    "CaGe_train_comps, CaGe_train_inds = obtain_NASICON_with_metal(train_comp_names, ehull_train_arry, ['Ca','Ge'])\n",
    "CaGe_test_comps, CaGe_test_inds = obtain_NASICON_with_metal(test_comp_names, ehull_test_arry, ['Ca','Ge'])\n",
    "HfZr_train_comps, HfZr_train_inds = obtain_NASICON_with_metal(train_comp_names, ehull_train_arry, ['Hf','Zr'])\n",
    "HfZr_test_comps, HfZr_test_inds = obtain_NASICON_with_metal(test_comp_names, ehull_test_arry, ['Hf','Zr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weighted ratio\n",
    "\n",
    "ratio =  best_clf.class_weight_/best_clf.class_weight_.sum()\n",
    "CaGe_LSGS_train_proba = best_clf.predict_proba(best_X_train[CaGe_train_inds])*ratio\n",
    "CaGe_LSGS_test_proba = best_clf.predict_proba(best_X_test[CaGe_test_inds])*ratio\n",
    "HfZr_LSGS_train_proba = best_clf.predict_proba(best_X_train[HfZr_train_inds])*ratio\n",
    "HfZr_LSGS_test_proba = best_clf.predict_proba(best_X_test[HfZr_test_inds])*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,7))\n",
    "x_min, x_max = best_X_train[:,0].min()-0.5, best_X_train[:, 0].max()+0.5\n",
    "y_min, y_max = best_X_train[:,1].min()-50, best_X_train[:, 1].max()+100\n",
    "\n",
    "plot_stepx = (x_max-x_min)/500;\n",
    "plot_stepy = (y_max-y_min)/500;\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_stepx),\n",
    "                     np.arange(y_min, y_max, plot_stepy))\n",
    "Z = best_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.binary,alpha=0.2)\n",
    "\n",
    "plt.scatter(best_X_train[CaGe_train_inds,0],best_X_train[CaGe_train_inds,1],cmap='pink_r',marker='o',\n",
    "         s=210,c=CaGe_LSGS_train_proba[:,0],edgecolors=['g' for _ in range(len(CaGe_train_inds))],alpha=0.4,\n",
    "            vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_train[CaGe_test_inds,0],best_X_train[CaGe_test_inds,1],cmap='pink_r',marker='^',\n",
    "         s=210,c=CaGe_LSGS_test_proba[:,0],edgecolors=['g' for _ in range(len(CaGe_test_inds))],alpha=0.4,\n",
    "            vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_train[HfZr_train_inds,0],best_X_train[HfZr_train_inds,1],cmap='pink_r',marker='o',\n",
    "         s=210,c=HfZr_LSGS_train_proba[:,0],edgecolors=['k' for _ in range(len(HfZr_train_inds))],alpha=0.8,\n",
    "         vmin=0.0,vmax=1.0)\n",
    "plt.scatter(best_X_test[HfZr_test_inds,0],best_X_test[HfZr_test_inds,1],cmap='pink_r',marker='^',\n",
    "         s=210,c=HfZr_LSGS_test_proba[:,0],edgecolors=['k' for _ in range(len(HfZr_test_inds))],alpha=0.8,\n",
    "         vmin=0.0,vmax=1.0)\n",
    "\n",
    "_ = plt.yticks([0.00/norm_factor,0.10/norm_factor,0.20/norm_factor,0.30/norm_factor,0.40/norm_factor],\n",
    "           [0.0,0.1,0.2,0.3,0.4])\n",
    "_ = plt.xticks(fontsize=24); plt.yticks(fontsize=24)\n",
    "plt.xlabel('$\\sqrt[3]{N_{Na}}$+($Q_{A}^{Std}$)$^2$',fontsize=28)\n",
    "plt.ylabel(r'E$_{EWald}^2$$\\cdot$X$_{M}^{Na}$$\\cdot$R$_{M}^{Std}$',fontsize=28)\n",
    "plt.xlim((-0.5,2.8))\n",
    "plt.ylim((-30,780))\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_5d.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
